{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the path to the project root dynamically\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from app.services.neo4j_service import Neo4jService\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "service = Neo4jService()\n",
    "service.reinit_graph(url=os.getenv(\"TEST_NEO4J_URI\"),\n",
    "                username=os.getenv(\"TEST_NEO4J_USERNAME\"),\n",
    "                password=os.getenv(\"TEST_NEO4J_PASSWORD\"))\n",
    "service.reinit_vector(url=os.getenv(\"TEST_NEO4J_URI\"),\n",
    "                username=os.getenv(\"TEST_NEO4J_USERNAMER\"),\n",
    "                password=os.getenv(\"TEST_NEO4J_PASSWORD\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"CHAT_GPT_MODEL\",\"gpt-4o-mini\"))\n",
    "print(os.getenv(\"NEO4J_GPT_MODEL\",\"gpt-4o-mini\"))\n",
    "print(os.getenv(\"LANGCHAIN_PROJECT\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect file paths\n",
    "files = [\n",
    "    \"./dataset/b0256024-4f01-4566-97e7-cd91b0500f47.pdf\",\n",
    "]\n",
    "\n",
    "# Open files and pass to the method\n",
    "doc = service.read_PDFs_and_create_documents(\n",
    "    [open(file, \"rb\") for file in files],\n",
    "    files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "\n",
    "def merge_documents(docs: List[Document]) -> Document:\n",
    "    combined_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    merged_doc = Document(\n",
    "        metadata={'source': 'merged_documents'},\n",
    "        page_content=combined_text\n",
    "    )\n",
    "    return merged_doc\n",
    "\n",
    "merged_doc = merge_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_split = await service.split_documents_into_chunks([merged_doc])\n",
    "len(merge_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"merged_split.json\", \"w\") as f:\n",
    "    f.write(json.dumps([dict(x) for x in merge_split], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "with open(\"merged_split.json\", \"r\") as f:\n",
    "    merge_split = json.load(f)\n",
    "    \n",
    "merge_split = [Document(**x) for x in merge_split]\n",
    "\n",
    "len(merge_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_split_translate = await service.translate_documents_with_openai(merge_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"merged_split_translate.json\", \"w\") as f:\n",
    "    f.write(json.dumps([dict(x) for x in merged_split_translate], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "with open(\"merged_split_translate.json\", \"r\") as f:\n",
    "    merged_split_translate = json.load(f)\n",
    "    \n",
    "merged_split_translate = [Document(**x) for x in merged_split_translate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_split_translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data into 10 chunks\n",
    "print(len(merged_split_translate))\n",
    "number_of_chunks = len(merged_split_translate)\n",
    "number_of_target_chunks = 10\n",
    "\n",
    "number_merge_chunks = number_of_chunks // number_of_target_chunks if number_of_chunks % number_of_target_chunks == 0 else number_of_chunks // number_of_target_chunks + 1\n",
    "\n",
    "new_chunks = []\n",
    "for i in range(number_of_target_chunks):\n",
    "    start = i * number_merge_chunks\n",
    "    end = (i + 1) * number_merge_chunks\n",
    "    new_chunks.append(merged_split_translate[start:end])\n",
    "    \n",
    "len(new_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(len(new_chunks)):\n",
    "    a += len(new_chunks[i])\n",
    "    print(len(new_chunks[i]))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {\n",
    "    f\"step_{i}\": [doc.model_dump() for doc in new_chunks[i]] for i in range(len(new_chunks))\n",
    "}\n",
    "\n",
    "with open(\"new_chunks.json\", \"w\") as f:\n",
    "    f.write(json.dumps(chunks, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "with open(\"new_chunks.json\", \"r\") as f:\n",
    "    chunks = json.load(f)\n",
    "    \n",
    "new_chunks = {k: [Document(**x) for x in v] for k, v in chunks.items()}\n",
    "\n",
    "new_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(os.getenv(\"NEO4J_GPT_MODEL\",\"gpt-4o-mini\"))\n",
    "\n",
    "chunk_token = {}\n",
    "\n",
    "for k, v in new_chunks.items():\n",
    "    tokens = 0\n",
    "    for doc in v:\n",
    "        tokens += len(encoding.encode(doc.page_content))\n",
    "    chunk_token[k] = tokens\n",
    "    \n",
    "chunk_token, min(chunk_token.values()), max(chunk_token.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"LANGCHAIN_PROJECT\",\"\"))\n",
    "print(os.getenv(\"LANGCHAIN_API_KEY\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graph_doc = []\n",
    "\n",
    "for step, chunk in new_chunks.items():\n",
    "    list_graph_doc = await service.convert_documents_to_graph(chunk)\n",
    "    all_graph_doc.append(list_graph_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_doc = {\n",
    "    f\"step_{i}\": [doc.model_dump() for doc in all_graph_doc[i]] for i in range(len(all_graph_doc))\n",
    "}\n",
    "\n",
    "with open(\"graph_doc.json\", \"w\") as f:\n",
    "    f.write(json.dumps(graph_doc, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_community.graphs.graph_document import GraphDocument\n",
    "\n",
    "with open(\"graph_doc.json\", \"r\") as f:\n",
    "    graph_doc = json.load(f)\n",
    "    \n",
    "    \n",
    "graph_doc = {k: [GraphDocument(**x) for x in v] for k, v in graph_doc.items()}\n",
    "    \n",
    "graph_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"LANGCHAIN_PROJECT\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"MATCH (n) DETACH DELETE n\"\n",
    "service.graph.query(query)\n",
    "service.graph.query(\"CALL apoc.schema.assert({}, {})\")\n",
    "Question_list = [\n",
    "    \"What role does the Provincial Electricity Authority play in renewable energy projects?\",\n",
    "    \"As a Renewable Energy Project Manager, how does the T-VER program ensure additionality in renewable energy projects?\",\n",
    "    \"How does the Thailand Greenhouse Gas Management Organization contribute to the development of renewable energy projects in Thailand?\",\n",
    "    \"What are the key conditions and applicability criteria for renewable energy projects under the T-VER program in Thailand?\"\n",
    "]\n",
    "\n",
    "for step in graph_doc.keys():\n",
    "    current_chunk = new_chunks[step]\n",
    "    current_graph = graph_doc[step]\n",
    "\n",
    "    # Add Document and Graph to the Neo4j\n",
    "    service.add_graph_documents_to_neo4j(current_graph)\n",
    "    await service.store_vector_embeddings(current_chunk)\n",
    "        \n",
    "    # Ask Questions\n",
    "    for i in range(len(Question_list)):\n",
    "        answer = await service.get_output(Question_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_time = datetime.now() - timedelta(days=7)\n",
    "\n",
    "runs = list(\n",
    "    client.list_runs(\n",
    "        project_name=\"test-scale\",\n",
    "        run_type=\"llm\",\n",
    "        start_time=start_time,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"name\": run.name,\n",
    "            \"model\": run.extra[\"invocation_params\"][\n",
    "                \"model\"\n",
    "            ],  # The parameters used when invoking the model are nested in the extra info\n",
    "            **run.inputs,\n",
    "            **(run.outputs or {}),\n",
    "            \"error\": run.error,\n",
    "            \"latency\": (run.end_time - run.start_time).total_seconds()\n",
    "            if run.end_time\n",
    "            else None,  # Pending runs have no end time\n",
    "            \"prompt_tokens\": run.prompt_tokens,\n",
    "            \"completion_tokens\": run.completion_tokens,\n",
    "            \"total_tokens\": run.total_tokens,\n",
    "        }\n",
    "        for run in runs\n",
    "    ],\n",
    "    index=[run.id for run in runs],\n",
    ")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Run.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Run.csv\").rename(columns={\"Unnamed: 0\": \"id\"}).set_index(\"id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values = df.values[::-1]\n",
    "\n",
    "all_chunk = list(range(10))\n",
    "\n",
    "GraphCypherQa_Token = [sum(x)/5 for x in df_values[::2, -1].reshape(10, 5)]\n",
    "LLMChain_Token = [sum(x)/5 for x in df_values[1::2, -1].reshape(10, 5)] \n",
    "\n",
    "GraphCypherQa_Latency = [sum(x)/5 for x in df_values[::2, -4].reshape(10, 5)]\n",
    "LLMChain_Latency = [sum(x)/5 for x in df_values[1::2, -4].reshape(10, 5)]\n",
    "\n",
    "print(GraphCypherQa_Token)\n",
    "print(LLMChain_Token)\n",
    "print(GraphCypherQa_Latency)\n",
    "print(LLMChain_Latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reverse the DataFrame values\n",
    "df_values = df.values[:50]\n",
    "\n",
    "# Select every other row for GraphCypherQa and LLMChain\n",
    "graph_cypher_qa_values = df_values[::2, -1]  # Adjust column index as needed\n",
    "llm_chain_values = df_values[1::2, -1]       # Adjust column index as needed\n",
    "\n",
    "# Set your desired chunk size\n",
    "chunk_size = 5\n",
    "\n",
    "# Calculate the number of full chunks for each array\n",
    "num_chunks_gcq = len(graph_cypher_qa_values) // chunk_size\n",
    "num_chunks_llm = len(llm_chain_values) // chunk_size\n",
    "\n",
    "# Trim the arrays to make their lengths divisible by the chunk size\n",
    "graph_cypher_qa_values = graph_cypher_qa_values[:num_chunks_gcq * chunk_size]\n",
    "llm_chain_values = llm_chain_values[:num_chunks_llm * chunk_size]\n",
    "\n",
    "# Reshape the arrays\n",
    "graph_cypher_qa_chunks = graph_cypher_qa_values.reshape(num_chunks_gcq, chunk_size)\n",
    "llm_chain_chunks = llm_chain_values.reshape(num_chunks_llm, chunk_size)\n",
    "\n",
    "# Calculate the average over each chunk\n",
    "GraphCypherQa_Token = graph_cypher_qa_chunks.mean(axis=1)\n",
    "LLMChain_Token = llm_chain_chunks.mean(axis=1)\n",
    "\n",
    "# Repeat the process for latency or any other metric\n",
    "# Assuming latency is at column index -4\n",
    "graph_cypher_qa_latency_values = df_values[::2, -4]\n",
    "llm_chain_latency_values = df_values[1::2, -4]\n",
    "\n",
    "# Calculate number of chunks for latency data\n",
    "num_chunks_gcq_latency = len(graph_cypher_qa_latency_values) // chunk_size\n",
    "num_chunks_llm_latency = len(llm_chain_latency_values) // chunk_size\n",
    "\n",
    "# Trim the latency arrays\n",
    "graph_cypher_qa_latency_values = graph_cypher_qa_latency_values[:num_chunks_gcq_latency * chunk_size]\n",
    "llm_chain_latency_values = llm_chain_latency_values[:num_chunks_llm_latency * chunk_size]\n",
    "\n",
    "# Reshape the latency arrays\n",
    "graph_cypher_qa_latency_chunks = graph_cypher_qa_latency_values.reshape(num_chunks_gcq_latency, chunk_size)\n",
    "llm_chain_latency_chunks = llm_chain_latency_values.reshape(num_chunks_llm_latency, chunk_size)\n",
    "\n",
    "# Calculate the average latency over each chunk\n",
    "GraphCypherQa_Latency = graph_cypher_qa_latency_chunks.mean(axis=1)\n",
    "LLMChain_Latency = llm_chain_latency_chunks.mean(axis=1)\n",
    "\n",
    "# Prepare x-axis values based on the number of chunks\n",
    "all_chunk_gcq = list(range(num_chunks_gcq))\n",
    "all_chunk_llm = list(range(num_chunks_llm))\n",
    "\n",
    "# Plotting the Token averages\n",
    "plt.plot(all_chunk_gcq, GraphCypherQa_Token, label=\"GraphCypherQa\")\n",
    "plt.plot(all_chunk_llm, LLMChain_Token, label=\"LLMChain\")\n",
    "plt.legend()\n",
    "plt.title(\"Average Token per Chunk\")\n",
    "plt.xlabel(\"#Chunk\")\n",
    "plt.ylabel(\"Token\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Data\n",
    "all_chunk = np.arange(1, 11)\n",
    "GraphCypherQa_Token = np.array([6669.2, 11202.4, 16648.6, 24328.0, 32995.6, 40034.2, 46521.0, 53022.0, 61487.2, 74885.2])\n",
    "\n",
    "# Reshape all_chunk to a 2D array\n",
    "x = all_chunk.reshape(-1, 1)\n",
    "\n",
    "# Model training\n",
    "model = LinearRegression()\n",
    "model.fit(x, GraphCypherQa_Token)\n",
    "\n",
    "# Define the upper limit for Token\n",
    "upper_limit_token = 128000  # You can adjust this value as needed\n",
    "\n",
    "# Solve for chunk where Token reaches the upper limit\n",
    "upper_limit_chunk = (upper_limit_token - model.intercept_) / model.coef_[0]\n",
    "\n",
    "# Display the result\n",
    "print(f\"The chunk where the token value reaches {upper_limit_token} is approximately {upper_limit_chunk:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_chunk_gcq, GraphCypherQa_Latency, label=\"GraphCypherQa\")\n",
    "plt.plot(all_chunk_llm, LLMChain_Latency, label=\"LLMChain\")\n",
    "plt.legend()\n",
    "plt.title(\"Average Latency per Chunk\")\n",
    "plt.xlabel(\"#Chunk\")\n",
    "plt.ylabel(\"Latency (s)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
